{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom nltk import word_tokenize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n# GET TRAIN, VAL AND TEST DATA #\n\ndef build_data(train_data, test_data):\n    train_data = train_data.values\n    test_data = test_data.values\n    \n    train_examples = []\n    train_labels = []\n    \n    test_examples = []\n    \n\n    for i in range(len(train_data)):\n        \n        train_examples.append(train_data[i, ...])\n        train_labels.append(train_data[i, 3])\n        \n            \n\n    train_examples = np.asarray(train_examples)\n    train_phrases = train_examples[:, 2]\n    \n    train_labels = np.asarray(train_labels)\n    \n    for i in range(len(test_data)):\n        test_examples.append(test_data[i, ...])\n\n    \n    test_examples = np.asarray(test_examples)\n    test_phrases = test_examples[:, 2]\n\n    \n    print(train_phrases.shape, train_labels.shape)\n    print(test_phrases.shape)\n    \n    return train_phrases, train_labels, test_phrases\n     \n\n\ndef get_label_distribution(train_labels):\n    \n    sns.set(color_codes=True)\n\n    dist = train_labels\n    print(len(dist))\n\n    dist_dict = {'0':0, '1':0, '2':0, '3':0, '4':0}\n\n    for j in range(len(dist)):  \n        dist_dict[str(dist[j])] += 1\n\n\n    print(dist_dict)\n\n    x = []\n    y = []\n\n    for value in dist_dict.keys():\n        x.append(value)\n    for value in dist_dict.values():\n        y.append(value)\n\n    fig, ax = plt.subplots(figsize=(12,8))\n    sns.barplot(x, y)\n    \n\n# PRE-PROCESSING TEXT #\n\ndef get_vocab(train_phrases):\n    \n    phrases = []\n    for i in range(len(train_phrases)):\n        phrases.append(train_phrases[i])\n\n    words = []\n\n    for phrase in phrases:\n        words.append(phrase.split())\n        \n    final_words = []\n    special_chars = [\".\", \",\", \";\", \"!\", \"?\", \"#\", \"&\", \"$\", \"''\", \"'\", \"!?\"] #found after seeing vocab words\n\n    for j in range(len(words)):\n        for k in range(len(words[j])):\n            if words[j][k] not in special_chars:\n                final_words.append(words[j][k].lower())\n\n    final_words.append(' ')\n\n    vocab_words = sorted(set(final_words)) #get a vocabulary of unique words\n\n    vocab = {u:i for i, u in enumerate(vocab_words)}\n    return vocab\n\n\n\ndef word2index(string, vocab):\n    \n    special_chars = [\".\", \",\", \";\", \"!\", \"?\", \"#\", \"&\", \"$\", \"''\", \"'\", \"!?\"] #found after seeing vocab words\n    idx_list = []\n    ws = string.split()\n    for w in ws:\n        if w not in special_chars:\n            w = w.lower()\n            idx_list.append(vocab[w])\n    return np.asarray(idx_list)\n\n\ndef pad_phrase(phrase, max_len):\n    phrase_len = len(phrase)\n    miss = max_len - phrase_len\n    before = np.zeros(miss)\n    \n    return np.concatenate((before, phrase))\n\n\n\ntrain_path = '../input/sentiment-analysis-on-movie-reviews/train.tsv.zip'\ntest_path = '../input/sentiment-analysis-on-movie-reviews/test.tsv.zip'\ntrain_data = pd.read_csv(train_path, sep=\"\\t\")\ntest_data = pd.read_csv(test_path, sep=\"\\t\")\n\ntrain_phrases, train_labels, test_phrases = build_data(train_data, test_data)\n\nget_label_distribution(train_labels)\n\nall_phrases = []\nfor i in range(len(train_phrases)):\n    all_phrases.append(train_phrases[i])\n\nfor i in range(len(test_phrases)):\n    all_phrases.append(test_phrases[i])\n\n\nvocab = get_vocab(all_phrases)\nprint()\nprint(str(len(vocab)) + ' unique words in vocabulary!')\nprint()\n\ntrain_encoded = []\nmax_len = 55\n\nfor phrase in train_phrases:\n    if len(word2index(phrase, vocab)) < max_len:\n        train_encoded.append(pad_phrase(word2index(phrase, vocab), max_len))\n    else:\n        print('PHRASE TOO BIG!')\n\n    \ntrain_encoded = np.asarray(train_encoded)\nprint(train_phrases.shape, train_encoded.shape)\nprint(train_phrases[0], train_encoded[0], train_labels[0])\n\nprint()\n\nprint(vocab)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import SGD\nfrom keras.utils import to_categorical\n\nvocab_size = len(vocab)\nembedding_dim = 100\nrnn_units = 32\nbatch_size = 32\n\nmodel = build_model(batch_size, vocab_size, embedding_dim, rnn_units)\nmodel.summary()\n\n\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['acc'])\nhistory = model.fit(train_encoded, to_categorical(train_labels), batch_size = batch_size, epochs = 10, validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch_count = range(1, len(history.history['loss']) + 1)\n\n\nplt.plot(epoch_count, history.history['loss'], 'r-')\nplt.plot(epoch_count, history.history['val_loss'], 'b-')\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_encoded = []\nmax_len = 55\n\nfor phrase in test_phrases:\n    \n    if len(word2index(phrase, vocab)) < max_len:\n        test_encoded.append(pad_phrase(word2index(phrase, vocab), max_len))\n    else:\n        print('PHRASE TOO BIG!')\n\n    \ntest_encoded = np.asarray(test_encoded)\nprint(test_phrases.shape, test_encoded.shape)\n\n\nprint()\n\ny_pred = model.predict_classes(test_encoded)\nprint(y_pred.shape)\nsub_file = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv',sep=',')\nsub_file.Sentiment=y_pred\nsub_file.to_csv('Submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}